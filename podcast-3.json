{"podcast_details": {"podcast_title": "Gradient Dissent: Exploring Machine Learning, AI, Deep Learning, Computer Vision", "episode_title": "Providing Greater Access to LLMs with Brandon Duderstadt, Co-Founder and CEO of Nomic AI", "episode_image": "https://artwork.captivate.fm/25fd1181-b46e-459b-85a5-d397eec4cdcf/JDLDW81K-wlJoAWL7ZnxLdTp.jpg", "episode_transcript": " Once you start dropping a number of parameters, you have to start being a little bit more intelligent about the curation of the data because you really only get so much data per parameter in these models. You're listening to Gradient Dissent, a show about machine learning in the real world, and I'm your host, Lukas Biewald. Brandon Duderstadt is the CEO and co-founder of Gnomek, maker of GPT for All, which is one of the fastest growing open source projects of all time. We talk about curating training data for LLMs, evaluating LLM performance in the wild, and he asked me for life advice. I hope you enjoyed this episode. Brandon, I feel like you're best known, at least to me, for being the creator of GPT for All, but you also run Gnomek, which is the company behind this. Could you give us a little rundown on what GPT for All is and what the company is and how you guys fit that together? Yeah, for sure. It's sort of a misnomer to call me creator. Really I would say my co-founder, Andre, was the one that really pushed it forward with help from some of the people in the open source community. GPT for All was originally almost just designed as sort of a weekend reaction to the GPT for paper. When that came out, we were pretty annoyed that it didn't have a method section and that there was this move towards science and less transparency into these models. For us, it was more about just trying to demonstrate to the community how we think science should be done out in the open so that people can see it and actually explore what is happening in these models and explain maybe why they're doing the thing they're doing. It started this weekend side project and has really kind of now, as it's grown in popularity, become full focus for Nomek, which is the company that Andre and I formed together. What does it do? What does GPT for All provide to an end user? I think that there's a variety of different value props and reasons why people seem to care about it. One is the fact that it's not just one model nowadays, but it's sort of this ecosystem of models. We see this really incredible outpouring of these great models from Wizard to Falcon to all of the llama variants and things like this. People want to be able to access these models and they might not always be technically capable of doing it. They might not have computational resources to do it. Part of the GPT for All initiative has really been collecting these models that industries are creating and groups are creating that are open source and compressing them so that people can run them on device and providing them this easy to use launcher so that even if you are not comfortable on a command line, you can go to the website, double click a thing and then get up and running with an open source language. Hugging Face is kind of well known for having a huge collection of models. How does GPT for All integrate with that? I would say pretty collaboratively. We like to work with Hugging Face. I love the people there. I think it's an incredible organization that's acted as a north star for us in terms of what we want to achieve with Gnome. You see a lot of the actual model files themselves in the GPT for All ecosystem get hosted on the Hugging Face Hub. Yeah, and honestly in the long term, I think Gnomek and Hugging Face have a lot of room to do some really interesting collaborations with not only the model hosting side of things, also on the data set side. But I guess today, what's the situation where I would go to Hugging Face to use a model versus go to GPT for All to use a model? So I think the overlap is the model file is stored on Hugging Face. That might not have say bindings where you can programmatically interact. Those are things that we provide and beyond that, it might not have an interface on Hugging Face. Right? Perhaps there's a space, perhaps there's not. If you do not have internet, you can't even access the space if it's there. And so we've got these files on Hugging Face that are somewhat inaccessible and GPT for All really builds this ecosystem around them to help one, programmers interact with it and to help people to be able to access it even if they're not as comfortable on the command line. And so what are the most popular models on GPT for All and why? Oh, man, it changes constantly. So we recently released a couple of models that have gotten really popular. One is we compressed the REPLIT code model so it can run locally. And a lot of people seem to be really attracted to the idea of having a co-pilot that doesn't require them sending their code to somebody else's model. I think we've seen some pretty high profile situations with Samsung and others where sending code to an external entity might end up firing on the engineers. And so a lot of people seem to be getting a lot of joy out of the REPLIT model running locally. And then the Falcon models as of recent, I think, are just the hottest thing. Before that, it was the MPT 7B. Before that, it was GPT for All J. And I think this trend will continue. The open source community will continue to really push the limits of these models and continue to close the gap with the closed source. You know, these language models, I think it's always tough. One question I always get everywhere I go is, hey, which language model should I use and why? I feel like you're in a great position to talk about that. What do you recommend if someone comes to you and says, hey, I want to play with a language model, which one should I use? It really depends on the downstream task and also the constraints of their use case. I believe in a future where there's going to be a proliferation of different models that are all good at different things. And it's going to be very individualized almost case by case what model someone wants to use. I think nowadays we're in this interesting micro environment where everyone is just experimenting. And so they don't know quite what the use case is. And so maybe the easiest thing is a massive model that can do everything. But as the roles that these models play become increasingly clear in these organizations, I think there's going to be a move towards using more domain specific models. As an analogy I offer, you wouldn't necessarily hire a PhD in neuroscience to do a clerical job, right? But that's effectively what's going on when you run GPT-4 to write copy. And so I think what's going to happen is as these use cases solidify, we're going to see a move towards domain specific models. Well, I mean, that analogy makes some sense. And obviously I've heard it before. But I think in a world where copying and intelligence doesn't take much, you might actually just want to use, I don't know, maybe the smartest person or something to do all your tasks, right? We don't see necessarily a proliferation of software products, right? People tend to gravitate towards the best software products in most domains. So it doesn't seem necessarily obvious to me that everyone would want a custom model. Can you say more about why you think there will be such a proliferation of models versus everyone gravitating to using the biggest model or the model trained on the most data? Yeah, I think there's a couple of reasons. The first is just the unit economics of running them. GPT-4 is very expensive to run and strides are being taken to compress it. But at the end of the day, smaller models are always going to be more efficient. They're always going to have better unit economics. So as the space solidifies and there's more pressure to actually get something that's more cost effective out there, I think you're going to see this movement to make paying an upfront cost to getting a model and then having that payoff in the downstream. I also think people are going to become increasingly wary of sending their data to a third party service. You hear it all the time in the startup community, oh, data is remote. And yet people are really sending away their data to these mega companies that are just promising not to compete with them. But I mean, we've heard a lot of promises from companies in the past. So I don't know how much I personally trust that. And so I think from a privacy and from an economic standpoint, there's going to be pressure to move these models on prem and into specialized applications. I guess you could also imagine a world where there's one best open source model that everybody uses. I mean, I understand there's an axis of model size and complexity versus performance. But I guess other than that, why do you think there'll be specialized models for so many different applications versus the kind of current trend which seems to be more prompt engineering one model to do the thing that you want? Yeah. So I think good sort of case study here is maybe like Falcon versus Revele. So looking at Falcon 40B is pretty great overall model at a variety of tasks, probably the best current open source arguably model. But for code and code completion and copilot tasks specifically, and in particular local copilot tasks, you're going to want a model that runs much faster. So it's going to have to be much smaller. And so that's the Revele 3. And then even beyond that, once you start dropping a number of parameters, you have to start being a little bit more intelligent about the curation of the data because you really only get so much data per parameter in these models. And so as the models get smaller, I think, and their domains become increasingly specified, people will start being more intelligent about what data they curate to actually train them. And that is what's going to enable some of these like edge real time, low cost solutions. What about the cost in training a model? Right now we see large organizations with tons of money open sourcing models like Nvidia and Meta and others. But it does seem like without an economic model, it might be hard to imagine the huge compute costs getting footed by someone who doesn't see a return there. Do you have any thoughts on how that might play out? Yeah, so training is such a broad spectrum of things that you can do. So you've got pre-training, you've got fine tuning, you've got RLHF. These are all almost broadly referred to an umbrella of training. And I think what you've seen over the past year or so is a strong move towards really efficient fine tuning techniques. So first you started to see things like adapters with LoRa. And now you're even seeing things like QLoRa where you can take a quantized base model, which is compressed and run very fast. And you can use that to train a very small set of adapter weights very efficiently for particular downstream tasks. And so I think there's going to be a place for base model training and open source still. And I think there will be a couple of main base models. One might be like a code based model, one might be say a creative based model, one might be a chat based model. And then I think individual organizations will adapt using these sort of more efficient, more cost effective fine tuning methodologies for their particular use cases. Can you say more about the state of the art of fine tuning right now and how that works? Yeah. So I would say the most, and I mentioned this just a moment ago, the most exciting recent development in my eyes is QLoRa. So I guess just going through roughly the history of LoRa, these are these sort of like weights, it's a very small number of weights compared to the number of weights in the model. You can just add them into the model, like literally add them in a couple of places to really modify the model behavior. The original LoRa paper makes some compelling arguments about when you actually do a full fine tune of these models, that is fine tuning all the weights, you get roughly the same solutions as just adding these adapters into the model. And so that's a radical improvement in efficiency. The problem with the original LoRa is you still actually have to run computation on the entire base model, quite large. Simultaneously with GML and these other sort of like quantization libraries that are popping up all over the place, what you're starting to see are people that are becoming cognizant of ways to run these base models much faster. And so these two trends intersected with QLoRa. And what that paper showed is you can take a base model that you've quantized and still use that sort of low cost quantized base model to train LoRa adapters, which massively decreases sort of cost from a compute standpoint of fine tuning these things, which is very exciting. I'm not sure that everyone listening to this will even know what quantization is. Maybe could you kind of start with what quantization is and what it does, and then go into more the details on like how to make it work effectively? Yeah. So I'll keep it relatively high level in that case. A lot of people joke around these machine learning models are big piles of linear algebra that you kind of stir to get an answer. It turns out that the actual data type of the thing that you're stirring are matrices with floating point precision in them. And this is just using say 32 bits or 64 bits of computer memory to represent a particular weight in the network. What you can achieve with quantization is a roughly equivalent sort of behavior of the network. But instead of those weights being 32 bit or 64 bit like floating point numbers, you get something that is like a four bit or an eight bit. And this is from a computational standpoint, much cheaper to operate. And that's where a lot of the efficiency gains. So are the efficiency gains just directly like, okay, we made these numbers a quarter of the size. So now the model's a quarter smaller. So it runs four times as fast. How big are the gains? You can get really significant gains. The exact curves are somewhat complicated because there's a lot of questions about what exact operations get called and some exact operations are more efficient than others. So one thing that comes to mind here is this idea of two-four sparsity, which is this particular way that you can add zeros to the weight matrix of a non-quantized model that can be taken advantage of by like the software of Nvidia at large to actually get like an efficiency gain. So there's like these very particular patterns that can induce that allow you to get these very particular kernel calls that help you make these things faster. But generally you see, that's a bit of a tangent. Generally what in the quantization space is you can get pretty significant. So for instance, for the REPLIT 3B model, we have that running at something like 25 tokens per second on a Mac. And that is for a 3 billion parameter model, that was something that was frankly like unheard of. What would it have been just running the raw original model? How many tokens per second roughly? If you can even load it in the memory, probably like a couple, like ones or twos. I see. And I guess I've heard even like quantization down to a single bit of being tried. Like how far can you take it before the model performance starts to suffer? So you definitely can do that. And binary neural networks are this super interesting active area of research that's been around for quite some time. Usually people have cared about these things since I got into the field in like 2017. But really the main idea of all of these quantization sparsity methods is the performance compression curve of a lot of these models is super advantageous. So you can in many cases compress a model say like 50%, almost no drop in performance. Now it gets quite steep in terms of performance drop off when you start getting to extremes like the binary and the 4 bit. But there are really great intermediary states where you compress it quite a bit and still maintain a lot of performance. And this points to I think one of the things that makes the sort of new, I don't want to use the phrase software 2.0, but the software 2.0 style computing paradigm so interesting is that your compute resources are very elastic. So you can really very finely tune the trade off between efficiency and accuracy. Whereas with previous code systems, if you wanted to get more efficient, it would require this sort of massive intervention into the structure of the code base. Whereas now you can just dial up compression or sparse. And that's a very interesting property of this new paradigm of software. I think it's been slightly under explored to this point. Now when you do this compression, do you need to also do it in the context of underlying training data or can you just take the model and compress it? Again, it's a spectrum. The techniques range widely. So you can get away with just taking the model and compressing it, but it won't be as good as if you can make decisions about the entire training and sort of inference process. And so the more control over the entire model creation process that you have, the better kind of downstream performance you're going to get because you'll be able to do things like say curing input data or design the model in an architecture that's more amenable to compression, or even say equip it out of the gate with something like a retrieval mechanism. There's a lot of really cool work that we can chat about on sort of like more efficient methods of retrieval that I think is going to become increasingly common in the next couple of months. I'm curious about fine tuning. We also see kind of a wide variety of these on that, right? Like notably GPT is hard to fine tune, kind of prohibitively expensive to fine tune the cloud-based models. But I would imagine your models are like typically fine tuned. Do you have any sense on if someone asks you, hey, should I fine tune or not? How you would advise someone to approach that question? Yeah. So I think the first question becomes like, what is your operating domain? What kind of data do you have? Do you know what you're really optimizing for? I think a lot of organizations right now are just trying to get the first model out there. And that's a very different problem from like the first engineering iteration cycle. I have a model deployed. I now have a data flywheel where people are interacting with it. Maybe I'm even collecting ratings of whether or not those interactions were good or those behaviors were valid. And so it would really depend, I think, a lot on the operating domain specifically, as well as how mature their AI deployment is. This is their first time getting something out there or if they have something deployed and they're starting to have that data flywheel. And so I guess your view is like, hey, get something deployed without fine tuning and then fine tune it over time. Is that sort of your general suggestion? I think that's right. And this harkens back to what I was talking about earlier. We really, I think, are in the early days of this, what is frankly a new paradigm of computing. Everyone is just trying to get something out there and see where it fits in their organization and see what works. And for doing that, having these massive generalist models that are at some level subsidized venture capital right now is really wonderful. But over time, it's become increasingly clear what particular roles those models are going to fill into the flow chart of information through an organization. And at that point, once you've deployed something and that role is solidified, it becomes time to start thinking about, oh, maybe I can fine tune a more efficient model to fill this role and really drop the cost of this thing and really start to get it to operate at a very high level of proficiency in my operating domain. And now GPT for all really came out of the gate with an absolutely astounding growth and kind of usage and GitHub stars and all that. Was that something that you tried to make happen or did you feel like it happened to you? No. GPT for all was very much like an accident in a lot of ways. Grew out of this weekend project that was born in sort of the frustration of the lack of transparency in the GPT for all. And I think the response was unlike anything we could have ever predicted. Yeah, I'm just really happy that people seem to care about it and seem to want to keep these models open. I think that in the long run, that is the best way that this can go. How does it fit into your plans for the whole company? Yeah, so I mean, it's definitely changed them. I joke around about how for a long time, it was a weird company. We had three employees and two products being Atlas, which is our data mapping engine, and then GPT for all, which is the open source language model ecosystem. But they're actually quite harmonious with each other. So first of all, GPT for all wouldn't be possible without Atlas. If people take one thing away from the original GPT for all model, it's like how important data curation is for downstream quality. And it shocks me to this day that there are practitioners in machine learning that don't have mental models of their training data before they feed it to these systems. Really, all model behavior is inherited from the properties of the training. And so knowing what is in there is really critical. Even beyond that, we are now able to loop and connect GPT for all style models back into the Atlas system. For example, with this one capability where you put in someone's structured data to Atlas, then we will automatically abstract like an ontology over it. So we'll give you class labels and then a structure about how those classes connect. And that's all powered by GPT for all style models. And so that is a situation where we can really start to provide exploratory value in terms of data exploration process to users by leveraging GPT for all style models. And I think what you'll see is increasingly these models will be used for subsequent downstream pooling. And this is in every application from data exploration applications to, say, video conferencing and podcasting applications. So before we get too far down this path, can you talk about what Atlas does? Yeah. So I will give you just the story of Atlas, roughly. In a sentence, Atlas allows you to do exploratory data analysis on massive unstructured data sets in a web browser. The reason why I cared about this was due to my experiences at RadAI, which was this generative medical AI company that I chopped out of my PhD to join in 2018. Essentially at that company, we were looking at applying transformers to radiology data. And one of the things that we needed was for the transformers that we were building to actually outperform radiologists on double blind tests. If we couldn't achieve that, then not only was there not a company, but it was unsafe to deploy. And so we really had from day one this impetus to make these models very accurate in their operating domain. And what we found is effectively every problem that a model had could be traced back to some underlying error in the training data for that model. And so we went through this incredibly arduous process of trying to do exploratory data analysis on the model outputs and the ratings that we were getting back and trying to attribute that back to the training data. And this was all in one-off scripts in Jupyter notebooks and like map plot visualizations and plot-leaf visualizations when we were lucky. But those kinds of systems don't scale to machine learning size data sets. I don't know if you ever tried to do a 100,000 point plot-leaf scatter plot and actually navigate it. It's not going to work. And so it became really clear to me that there was this need for tooling around visual analysis of these massive data sets and that it would be incredibly high impact in five to 10 year horizon as people started to deploy these systems in fields like medicine where they have to be accurate. And so that was sort of the genesis of building Atlas. And so now with Atlas, we're pushing scales of like 20 million points. The bottleneck for us right now is just the time it takes to plug all the holes and scaling it. I think we can go much larger. We've done billion point scatter plots over the night sky that we'll release at some point. But doing it on the actual vectors coming from these models requires a little more of a leg. But I'm happy to see that we've upped the order of magnitude to operate on it. I'm happy to hear that data sets still matter for ML. I mean, that was always my background and interest. I'm curious when you were building the first data sets for GPT for all, how did you use Atlas to curate those data sets or look at those data sets? What kind of visualizations do you really need and what kind of choices are you making around building a high quality data set? Yes. So if you go look in the technical report, you'll see like a before and after snapshot of the data in Atlas. The first thing is just so for an illustrative example with GPT for all, there was a lot of data in a particular subset that was very short responses. It was three or four word responses. And I think the prompts were something like, here is an article. And then you're asking the model, what is this article about business or art or sports or something? The model would be like, this article is about business period. If you train your model on a ton of that data, it's going to produce short responses. You're teaching it to answer in a very concise way. First of all, realizing that's a thing that's happening. And second of all, like actually modifying your data set to remove those sets of things is I think quite important. Another thing is the amount of kind of deduplication and the semantic coverage of your data set. You want to make sure that you've got, if you're making a general purpose chatbot, say a wide variety of topics represented and that those topics occur in roughly equal concentrations. So for instance, if you happen to pull in a bunch of different data sets and one of them is much larger than the others, your model is going to spend an outsized capacity fitting to that topic and that's going to hurt its ability to speak to the rest of the topics in the data set. And so there's this sort of balancing you play by concentrating different topics in this mixture that's quite important. So I could imagine in a huge data set, it would be hard to find these things. There's these short responses. How are you using Atlas to find these situations? Yeah. The first step is just giving people an interface where they can get a sense of a lot of data at once and rapidly iterate. Pre Atlas, people were literally opening up text files or looking at the head of a pandas data frame. And you get maybe 10 samples of data or something in a spot check. And perhaps maybe once in a while you'll get like some aggregate plots. I think one of the first keys is Atlas shows you all the data on one screen at once. And then we start to have these Google Maps style, like annotations floating above the map describing what is in different places. And from those, you can find very obvious things. So one example that I talk about a decent amount here is the Anthropic HH data set. So if you look at in the open source model domain, there's this model called Koala that came out of Berkeley. And one of the things that they did was they did sort of language modeling on the chosen responses of the Anthropic HH data set. If you pull up that data set in Atlas, there's this sort of massive label that's just right in front of you in the top of it that just says like, and that's like a pretty obvious flag of, oh, maybe there's something here we don't want to train our language model on. And the reason why that data is included in the Anthropic HH data set is a little bit subtle. It turns out that data set is for building a reward model for RLHF. It's not for language model. But these are the kinds of errors that are really easy to miss if you're not building a mental model of your training. And it's very easy to have things like that slipped. And how are you generating these descriptions of what's inside the data? GPT for all. So this is what I was talking about earlier when I say we will use the inherent geometry of the data, of the embeddings of the data, to cluster it into different concepts and build this linkage on top of those concepts. And then we can use GPT for all style models to actually summarize those concepts in like a human interpretable way. And so what results is essentially a Google Maps style view of your data where you have these high level ideas labeled floating. And then as you dive in, finer and finer grains kind of categories start to emerge. And now is there anything different that you do in training a code model versus training a language model? Yeah. So training code and language models is actually very interesting because it's only been sort of recently realized in a widespread way that including code improves the language modeling capabilities of non-code models. And so there's really this open question right now that we're very interested in, which is how does the mixture of data in the training set of a model affect its capabilities? It becomes quite difficult to actually tease this out because the mechanisms of these models are unknown. But some of the work that we've done here is this idea of causal intervention where you take a training set, train a model on all of it, and then you take that same training set, ablate part of it, remove part of it, train the exact same style of model on it and look at how their representations differ. This is something that we did in a recent paper we published to Archive, which you can find on our website. And essentially what you find is there is this sort of like interesting semantic bleed effect. So one experiment we did is we trained a BERT model on DBPDO, which is this encyclopedia data set. And then we trained in a bladed model, same BERT model, but a bladed data set, I should say, on the DBPDF14 data set without plant articles. And what is those two models not only have a differing view of what plants are, but they also have differing views on how they represent animals and natural places. And so there's this interesting kind of like semantic diffusion effect where removing data of something that is kind of related to these other classes, but not exactly the same changes the representations of other things. And so teasing this out, I think that- How would you know that? Like how do you know that the model is different sense of what animals are when the plant data is removed? Like what does that even mean? Yeah. So getting super technical, one thing that you can look at are the associations that the model is making. And you can roughly map this to how close two representations that a model makes of two pieces of data are to each other. And what we find is that if you use that proximity as a measure of relatedness, when you ablate sort of plant articles from the training data, all of the proximity relationships in the natural place and animal articles start to get messed up as well. So two animals might be kind of far apart from each other. Yeah. So one of the first things that Nomek ever published was this article at VizXAI, which is an awesome sort of conference that everyone should aspire to a publish at, I think. It's a great inspiration to me at least, where we showed that there's this super weird property of BERT embeddings of Wikipedia, where the Ferrari is closer in representation space to an M1 Garand rifle than it is to Enzo Ferrari, the human that the car is named after. And so these sort of very interesting quirks of how the model is defining relatedness can really show up and wreak havoc on these downstream systems. How do you evaluate an embedding? It seems like I don't really have an a priori feeling about which thing the Ferrari should be closer to. Probably it depends on what you're trying to do. Do you have some metrics for saying if an embedding is good or not? I guess at the end of the day, it's like, is it useful or not? Right. I think it's very much an analysis that we run to see what associations are making out of the box. A lot of people take these, what are now being called foundation models, and kind of just apply them to whatever task they can throw at it. And sometimes the associations the foundation model has learned are good for that task, and sometimes they're not. But certainly we should know what they are so we can evaluate if they're good or not for the task. And it turns out that the process of fine tuning these models effectively updates those associations so that they happen to be good for the task. And do you quantify the quality of the embeddings themselves? So one of the challenges of model evaluation, we'll say, is defining that kind of downstream metric. And that really depends on your domain. One of the things that we endeavor to do is find ways to characterize the behavior of embeddings that are not sensitive to this downstream metrics so that they can be applied in various different capacities. And one of the things that we happen to look at is this idea of concept proximity, right? And so, for instance, if you are in, going back to the Ferrari example, if you're in an operating domain where you're maybe trying to classify artificial versus biological, perhaps that's a good association to be made. But if you're in an operating domain where you are maybe trying to classify the geographic origin of an article, that's probably a terrible association. And so it really depends on the downstream. And I guess you have a leaderboard of these models, right? There's lots of different ways that people evaluate these models, like Hugging Face has a leaderboard. There's plenty of benchmarking data sets. How do you think about evaluation today? Yeah, I think there's a lot of work to be done. It's very subtle. We use the Aluthor AI evaluation harness just because it's roughly standard right now. But obviously, it is not a complete characterization of the behavior of them. My kind of dream in the long term for machine learning is that it turns into something that's more like a taxonomic science. And what I mean by that is instead of studying an evaluation of one model at a particular point, we start to build family trees of these models and perform investigations on particular models that sit in strategic places on that family tree and try to generalize our findings about those things to families of models. So you can imagine a whole field of science that is based around measuring properties of machine learning models and then trying to see how they diffuse through the family tree. If fine tuning is rampant, you also might really have like a family tree or you could define clades of models, I suppose. Yeah. So one of the things I keep referencing this paper, it's called Comparing Foundation Models Using Data Kernels. Myself, we had a research intern, Hayden Helm, and Johns Hopkins professor, Kerry Preeb. One of the things we do in this paper is we show how you actually construct this family tree for a particular set of models, constructed set of models. And what we essentially demonstrate is the family tree in some sense is very related to the concentrations of different concepts in the training of these models. And so as you like say up with plants in your training data and you down with artists or natural places, it like pushes you towards a different family of model makers. Do you also do RLHF on your models? And how does that fit in with fine tuning? Like do you view that as a kind of a specialized type of fine tuning or? Yeah. So RLHF is in my mind fine tuning when it's hard to define your objective function. So if you can get say gold standard labels, so maybe let's take the classic cat dog example. It's pretty easy to get unambiguous cat dog labels on a data set. If it's only pets, you can just go to the standard fine tuning approach. The thing that's difficult about people is that it's often very confusing to try and tease out what they want. And so what RLHF does is it first builds a model that tries to predict what people want, almost a model of that objective function, and then trains a model to maximize the reward according to that objective function. It's not something that we do. One thing that we are interested in is understanding how RLHF impacts the embedding spaces of these models. And I'm happy to talk more about that if you want to go in that direction. But I think it's a good tool to have in the case where your objective function is difficult to access. And do you actually do that with the models that you generate? We do not. So we did not do it on the GBT for all or the GBT for all J models. And a lot of the open source models nowadays are not doing it. How come? I guess we haven't needed it. Interesting. Do you think we should be doing it? I mean, my understanding is that RLHF also allows you to kind of nudge the models towards kind of a more useful mode of interaction. And so I wonder if that type of nudging could make the models feel better. Like one thing that I observe, you know, enthusiastic user of lots of these models, is that the GPT-4 model, a lot of models claim to have very similar benchmarks to GPT-4. But when I use GPT-4, it feels more immediately useful to me. Kind of like the opposite for me of using Mid Journey, which feels like awesome, versus Dolly, which feels like a little bit lame. And I also play with them, you know, with my daughter, who's three. So she has like maybe just a more beginner mind on this stuff. And she really loves the interactions with GPT-4 and Mid Journey, actually, because I think maybe they just kind of spent more time pushing them into like a more, I don't know, like kind of friendly mode. So I guess it obviously depends on what you're doing, but I'm just kind of curious about the thinking there. Yeah. I'll give you an interesting anecdote, which is the flip side of this. A lot of people in the AI writing communities refer to models that have gone through RLHF as lobotomized. And the reason why is because they become unable to produce certain genres of content. So I remember I was at this event in Brooklyn recently called Word Hack, where all of these sort of artists that use these models for various sort of random projects get together and like talk about what they're doing. And one of the people was lamenting that they couldn't write villains using GPT-4 because the villains would always realize the error of their ways immediately and then go and be good people. And so it just depends on what you want. Right. So like RLHF has been highly effective at nudging these models toward being like the ideal sort of corporate chatbot. And like definitely a place for that. That's probably a very viable use of these things. But I think it's such a small slice of what is possible with these models. And I think it's a little bit foolish to think it's the only thing that we should. Yeah, totally fair. I guess maybe there's a different kind of RLHF. I agree that GPT-4 has kind of an annoying Boy Scout voice that's now totally recognizable to me when people are sending me emails generated by it. I completely can tell. But I wonder if that's like an artifact of the particular reward function that it's being given versus the process itself. Yeah, I'm sure it is. And I think this speaks more broadly to what I think will become sort of an industry level trend in that you'll have different foundation model providers that are interested in cases catering to different sort of groups of users. And so you'll have the foundation model provider that is maybe the best corporate chatbot. And that will probably be Anthropiq or OpenAI. And then there's I think room for providers that want to be the best for say creative writers. And this sort of gets back to this idea of a proliferation of models, right? There's a vast number of people that want to do a vast number of different things. And one optimization I think is not going to serve them. When you think of the open source models, I feel like most of them have come from still big corporations. Do you think there's room for startups or smaller companies to be building their own open source models? How would that actually happen? Yeah, well, if you look at kind of the most original one, it was very grassroots, basically GPTJ, right? Which I think the exact quote was like, on discord, someone was like, let's non ironically give OpenAI a run for their money. Like it started as a meme. And then it worked, right? And now a Luther is like a serious nonprofit organization doing really good work. And so I think it's feasible that open source is able to build some of these really great base models. I think it becomes challenging, especially as the race for compute heats up. You see every day a new hundred million dollar round in the sum team of engineers because they're going to spend 80% of it on GPUs. And so the quip has become AI as a function from venture capital to Jensen and Nvidia, which I think is probably pretty real in a lot of senses. That really shuts down the access for sure. But I think this is why I'm interested in these more efficient training methodologies because there's a lot that you can do, like a hacker can do at home with commodity GPU or maybe even a CPU soon with a quantized base model and some of these things like QVora. And I guess, can you say more what you can do because like just a 4x or like a 20x or even like a hundred X speed up doesn't really seem like a hacker at home could really do much to compete like these gigantic model. Like what really could you do? I don't think we are going to see in the near future in open source GPT-4, right? It's too big. It requires too many resources, requires too much collaboration. When I think back to the early models that we built at RAD and we were beating doctors on double blinds with models in the ones of billions of parameters because that was like big at the time, which is it's weird to look back at that time. And so if you can have a model do something as complex as medicine at the 1 billion parameter level, if you, like I said, start to define the particular use cases of these models, I think you can cover a wide variety of the task space with a little bit of intent. Do you think it's important as part of your business model to train models yourself? It's an evolving question. And I mean, look, Nomek is a super early company. We've existed for less than two years and so our business model is also evolving. We've certainly talked about it. We have some conversations with computing like providers that are excited about the idea. It's unclear to me right now if we need another open source based model, right? We have a wide variety of really great ones like between Falcon and MPT in particular. Yeah, I think in some sense, it's not worth emitting the carbon to make another open source based model trained on the pile. There's been some talk of like, okay, well, if we can get a compelling enough set of tokens and use case, maybe it is worth doing it because we're serving some population that does not have access to a model like the system right now. So it's an evolving question. I guess one organization you haven't mentioned that seems to be funding a lot of this is Stability. Do you have thoughts on their role? I've only interacted a little bit with Stability. I don't have a ton of thoughts on their role. When I mentioned Aluthor, Stability provides a lot of their compute and I think Amad sits on their board even to this day. And so in some sense, they're proximal. But yeah, I think their role is definitely evolved in the community. Do you have thoughts on prompt engineering or chaining prompts? How do you think about the impact of that versus fine tuning models? Yeah, so I do have thoughts on this. The first thing I'll talk about is prompt chaining. I think something that people are only now starting to realize at large is that if you have a task, so a box and like the flow chart of things that you want to do, and you have multiple calls to a language model to achieve that task, the performance goes up. So the first kind of easy example of this is something that people are calling validators nowadays. So you ask the model to complete a task, and then you ask it to check its work. It turns out you can detect what is wrong, which at the time your performance goes way up. And in general, I think you're going to see this trend of people starting to call the base models more and more for any particular task. And a recent paper that talks about this that I liked a lot is something called Tree of Thought Prompting, where you've got your classic chain of thought, which is one branch of reasoning, so to speak. What Tree of Thought does is it says, well, let's have multiple hypotheses, multiple chains of thought being run in parallel with a validator prompt. So I can look at each of the chains of thought and evaluate whether or not I as the model, I'm going in a good direction. And that approach is incredibly powerful. And so I think we will continue to see prompting and chaining techniques proliferate more widely. And this is why things like Lang chain is really exciting to me for this reason, because I think you will need a really clean orchestration layer over these things. And you also see things like the Generative Agents paper from Stanford, which starts to integrate things like external memory and reasoning and reflection into these models. And these are all almost this higher level of abstraction over calling the model that can lead to these massive gains in capabilities and interesting emergent behaviors. And so I think all of that is going to be explored in great depth with great reward in the very near future. And if you were trying to make an LLM application today, how much energy would you spend on fine-tuning the model itself versus getting the right prompt to send into the model? I would spend, this is kind of a non-answer, but I would spend all of my energy trying to define as exactly as I can what role the model is playing in the flowchart of information that I'm trying to intervene in. Because only with a clear understanding of what box in the flowchart is that model filling will you be able to make these. Can you give me some possible answers to that question? Give me an application that you've seen and how you would answer that for a common application. Yeah, so I would say in the context of, let's look at Nomek's auto labeling engine. The first pass of this thing is prompt engineering and that gets you quite far. What is the application here? Yeah, so one of the things that Nomek does that we were talking about earlier is put this text on top of your map that kind of orients you to what is in your data, sort of the Google Maps level view when you're in the database. And the first iteration here is you just prompt the model and you try and engineer that prompt. But then you'll get this data flywheel where you'll start to notice like, oh, these are good cases, these are bad cases. And so once you start to really crystallize what you want out of that model, you can then go back and fine tune it and then that makes a massive difference. So I think maybe the abstract answer to your question is you probably want to do a proof of concept with prompt engineering. And then once you have your data flywheel set up, that's when you want to start out fine tuning. And in that process, how are you evaluating if things are getting better? Feedback from users, feedback from us internally if we find it useful, feedback from the people that are using Atlas every day. Sometimes it's like quite obvious. In the early iterations of these systems, they often make several quite obvious errors. So yeah, it's really setting up the data flywheel and getting that quality loop closed is what you'll want before. I totally agree. The qualitative feedback, especially on is really critical and people often forget to do that. But at some point, if you're fine tuning and you're trying to make iterative improvements, you need some quantifiable metric or set of metrics that you can look at to make sure you're not fooling yourself or just sort of like you yourself looking at part of the space that isn't really representative of what the average person would be looking at. Yeah. So and this goes back to our conversation about RLHF. In a situation where your quantifiable metric is obvious, say accuracy, you're going to want to just use that metric. When your quantifiable metric is not obvious, say you're only getting user ratings or user feedback, you're going to have to first train a model of what people like and then optimize against that model. And that is really like the innovation of RLHF. Do you have like a quantifiable metric for these labels? That's not something I can share. Interesting. Cool. Okay. Metric definition is a big part of the problem for a lot of. I'll actually, okay, let me tell you one of my favorite stories. So the first NeurIPS I ever went to, oh no, I think it was the second NeurIPS I ever went to, it was in Canada that year. I was a young starry eyed new PhD student kind of sitting there trying to figure out what cool things I wanted to see at a poster session. And Oriol Vinyls walked by, who was essentially, and I was like starstruck. I was like, oh my God, it's Oriol Vinyls, I need to go talk to him. So I run up to him and started talking to him a little bit. And at one point I just asked him for life advice. I like do this with interesting people that I meet. And I'm interested in what you might say to me as well. And he ponders for a minute and he goes, you should know your objective function. And I think that's one of the wisest things anyone has ever said. Wow. Do you know your objective function? I think it is constantly evolving. Okay. On June 30th, 2023, what is your objective function? The objective function for Brandon the human, the objective function for Nomic. We have to be very specific. I feel like the advice is to know your objective function as a human. I'd be curious about the objective function for Nomic, but I may be more curious about the objective function as a human. Yeah. So Nomics is pretty straightforward because it's on our website and we've thought quite a bit about it. It's improve the explainability and accessibility of AI and do what we can there. For Brandon the human, it becomes slightly more complicated because as a physical being, I have needs like being engaged in my life and getting sustenance and providing for people in my support network or my community. And so there's this additional term that you have to tack on to the Nomic mission, which is ensure that Brandon and his community are also flourishing. Right. So it's basically the Brandon objective function is maybe 70% Nomic objective function plus 30%, some level of personal and community fulfillment. Do you know your objective function? I would say I think about it a lot. So I don't think it's necessarily bad advice. I guess being clear about what you're trying to do and being practical about trying to get the things that you want. I guess I do think that that's good advice. For me, I break it into years. Every year I think about what's my theme for the year, which is maybe more than a year and more of like a nudge of what I'm trying to do than sort of like radically sort of the derivative of the function that I'm trying to change versus the function itself, if that makes sense. Every year you take a gradient step? A gradient step, exactly. That's what I try to do. So this year I've been trying to learn more actually, which is doing this interview is a good example of like things I'm trying to move in that direction. Yeah, like last year I was trying to enjoy the growth of my company more. I mean, again, these aren't the only things I care about. I have a family and kids and lots of things that are like kind of probably higher weight, but I feel like it's in the deltas that I can maybe make little steps to improve the function. And then the question still stands for you. Do you have any advice for me? Well, I guess I see you as something like a really exciting company that's growing. And so I think my main advice, more professional, that's kind of where my brain goes or something like I might be able to offer. And like all these things sort of like feel like cliches, I guess. Maybe I don't have anything quite as pithy as your objective function. I think one thing that's hard about having a fast growing startup is that the important things to do change at sort of each place that the company goes. And I think that, you know, you can get really bad advice from people when they think you're in a different phase than where you're at. Like when you're small, it's like, man, you really just want to ignore everything and make a good product. Like worrying about OKRs, worrying about anything else is such a trap. And so I think it's easy to get in this habit of like, man, these lunatics are giving me terrible advice. If you talk to a CEO of a bigger company, you'll just be like, I don't get it. But then I think actually, you will get to a stage where I think running the company well becomes a much more important thing to focus on. I think like showing up well for your company starts to get really, really important, maybe more so than any individual contribution that you can make to the company in most cases. So I guess that would be my high level professional advice. But you know, now that you kind of established yourself more, and I think a lot of our listeners are in college, you know, from the ones that reached out to me, I don't know, do you have any advice to a younger version of yourself? Or what if you walked by somebody at NeuroPy? They grabbed you by your paper and asked you for advice. What would you tell them? I would say bias towards building. You know, I look back at how Gnomef got created and like where the skill set to actually build these things came from. And certainly like the standardized education I've had helped. But the thing that by and large was the most useful was just like spending time hacking on side projects, going to hackathons. Maybe the concrete thing is just go to more hackathons and just actually get your hands dirty like doing things. And these things don't have to be great. Like often they probably won't work, but create something that like zero to one process has such an incredible like ability to make you learn and really start to grok systems that the more you invest in that, I think the better off. Yeah, that's funny. Like for me, I find it hard to learn really. You know, I get nervous interviewing someone like you when I haven't actually used your library. Like I feel like my younger self would have just had time to like use every single library. And it's scary to me when you just sort of like look at something, but you haven't actually tried it. I just feel like I wish I had a more efficient process for learning things, but I really don't. Like even with my own product, when we launch something and I haven't used it myself, it gives me a lot of anxiety until I'm actually engaged with it like as an engineer. And most of it is like fighting with error messages that I don't understand. I think 90% of the building process these days, but somehow I feel like I need that suffering to feel like I actually understand something. I think that's right. And hearkening back to this idea of how companies change over time, a new thing for me has been no mix getting to the point where I can't keep the entire state of the company in my brain at once. And that has been very scary, right? Like back when we were a couple of people hacking on a code base and touching every part of the system, like all the early engineers at Nome were full stack and so everyone touched every... I could tell you exactly the state of every partnership. I could tell you exactly the state of every employee. I could tell you exactly what's in every file on the code base. And that was wonderful. And now it's just infeasible to like allocate my attention such that I can keep up with it, right? Especially in open source. I wake up in the morning and there's just like another 50 pull requests. I'm just like, oh man, this is another thing that's impressive about Langchain is like the pace at which they keep up with the community. Their ability to just continue aggregating everybody's opinion at such a rapid cadence is really... It really is amazing. It reminds me of actually good advice that I got that I'll pass along to you and to anybody listening. I talked to a lot of CEOs running bigger companies to try to learn from them and mostly the advice is kind of hard to use. But one practical story that I think wasn't even framed as advice was, you know, Lou Cern, the CEO of New Relic, when it was like a public company, he told me that he spends one week every month basically by himself coding. And that was like a real wake up call to me just as someone who runs a much, much smaller company who feels overwhelmed by the day-to-day work and probably spends less of the time doing projects to at least take like once a quarter to get like an under-upped block of time to work on something. I think really served me well. Yeah. I might try and implement that. I always joke that my GitHub commit history tells the story of my startup better than any other piece of media could. Because in the early days when it's like a hack project before anyone else was on board, you just see two green strips on Saturdays and Sundays, right? And then exactly when I left my full-time job to go all in on it, because it's the green block, you can tell when we raised seed because there's like a little white sliver in the middle of the green blocks. And then it goes back to green. And then something very interesting happened a couple of months ago now with the launch of GPT for all where suddenly it became very white. And that is something I've been learning to try and deal with. Nice. You know, we always end with two questions. I want to make sure I get them in. What do you think is currently an underrated topic in machine learning that you think people should pay more attention to or you would like to spend more time on? That's a really good question. I think it's starting to become more popular, but policy in some sense. I was at this event the other day and talking to someone that does policy at OpenAI. And one of the things they said that resonated with me most was this idea of how economically empowering and disempowering this technology is going to be. And so I think trying to really take a moment and understand how we can implement these tools in a way that is maximally empowering to everyone is going to be increasingly important. There's a great book called Player Piano by Kurt Vonnegut that feels very appropriate for this time and that I think about quite a lot because it feels like we're moving into that world. I don't know personally what the solution is. My conjecture is that giving as many people as much access to these models as possible is the best way to try and allow them to reap the benefits of these things. Interesting. Do you have any papers on policy that you think are interesting? Policy is obviously so, so important. But the thing that's hard for me about engaging with policy is it's hard for me to find stuff where it feels tangible or useful. Do you know what I mean? I mean, no doubt, what could have a higher impact than creating better policies around all the big changes that are coming to our society? I find it hard to find stuff that I can really sink my teeth into. Do you have any recommendations there? No. I mean, I think this is one of the reasons why I cited this being under explored because all of what I've learned about it is through conversations with people that work at these places that I happen to have access to. And I'm glad they're at least thinking about the problem. And I think this is going to sound like a broken record, but I think going back and engaging with Player Piano by Vonnegut is the best thing that you can do in terms of getting intuition for the cautionary tale. This is the case that we need to maybe think that we should be avoiding this new technology. And it's funny, I read that book as a teenager. I'm trying to remember it. I think it's like almost nobody has jobs at that point, right? And somebody builds an automation system and automates himself out of a job and then they don't have a job, right? Like they get some bonus and it's kind of dystopian in that way, right? That's the cost. Yeah. The main premise of the book is it details a society where technology has progressed to the point where most people cannot work on the automaton. And so there's this very small number of hyper specialized people that are actually functionally able to work on these things. And everybody else is just sort of like pushed to the outskirts of the manufacturing facility. And there is a character that at one point actually does automate themselves out of a job as I recall. And we need to figure out how that does not become the case, right? We need to figure out, make sure that these tools are being integrated with people as opposed to replace. Okay. And my final question, a little bit of a different direction, I guess, is when you're trying to get these models to work for some useful tasks, what's the biggest practical challenge? It's another good question. I think I'm going to give you two answers. Specification is very important. And I think we've hit on that a lot. Really having a clear idea of what exactly you want the model to be doing, what exact role it's filling is very critical. And then visibility, right? These systems are still, their mechanisms are widely unknown. And so getting as accurate of a mental model as possible about what is actually going on so that you can start to reason about it is, I think, highly critical and really was the impetus for the first generation of this. All right. Well, thanks so much, Brendan. Real pleasure to talk to you. If you're enjoying these interviews and you want to learn more, please click on the link to the show notes in the description where you can find links to all the papers that are mentioned, supplemental material, and a transcription that we work really hard to produce. So check it out. Thank you so much for joining us, and an invite to take the panelists online last."}, "podcast_summary": "Gradient Dissent is a podcast about machine learning in the real world. In this episode, the host, Lukas Biewald, interviews Brandon Duderstadt, the CEO and co-founder of GnomeK, the company behind GPT for All. GPT for All is an open source project that aims to demonstrate transparent science and explore the inner workings of language models. Duderstadt discusses the value of GPT for All, the collaboration with Hugging Face, and the different models available on the platform. He also talks about the importance of curating training data, evaluating model performance, and the future of fine-tuning and quantization in the field of machine learning. Both Duderstadt and Biewald share their advice on topics such as objective function, prompt engineering, and policy in machine learning. Overall, the podcast provides insights into the challenges and opportunities in deploying and using language models in various applications.", "podcast_guest": "Brandon Duderstadt", "podcast_highlights": "Highlights from the podcast transcript:\n\n- GPT for All started as a weekend project to demonstrate transparency in science and has now become the main focus of the company Nomek.\n- GPT for All provides an ecosystem of models that can be accessed and run on devices easily, even for those who are not familiar with the command line.\n- Collaboration with Hugging Face has been collaborative and there is potential for more collaborations in the future, both in terms of model hosting and data sets.\n- The popularity of models on GPT for All changes constantly, with the most recent popular ones being the REPLIT code model and the Falcon models.\n- The use of more domain-specific models is expected to increase as use cases solidify and organizations see the need for models that are more tailored to specific tasks.\n- Smaller models offer better efficiency and economics, which is why there may be a proliferation of different models for different applications.\n- Fine-tuning techniques, such as adapters and quantization, have been developed to improve efficiency and cost-effectiveness.\n- Quantization reduces the precision of weights in a model, making computations more efficient, and can lead to significant speed gains.\n- Fine-tuning methods, such as QLora, allow for more cost-effective fine-tuning of models and can lead to improvements in efficiency and performance.\n- Evaluating embeddings is done by studying the associations and proximity of different representations in the embedding space.\n- The objective function of GPT for All is to improve the explainability and accessibility of AI.\n- The author suggests that it is important for individuals to know their objective function and to ensure personal and community flourishing.\n- The author recommends biasing towards building and spending time on side projects to learn and gain a better understanding of systems.\n- Policy is an underrated topic in machine learning, and understanding how AI can be economically empowering and disempowering is important for the future.\n- The book \"Player Piano\" by Kurt Vonnegut is mentioned as a cautionary tale and a way to gain intuition on the impact of technology on society.\n- The challenges in getting models to work for useful tasks include having clear specifications and visibility into the mechanisms of these models."}