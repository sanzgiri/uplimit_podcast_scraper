{"podcast_details": {"podcast_title": "Latent Space: The AI Engineer Podcast \u2014 CodeGen, Agents, Computer Vision, Data Science, AI UX and all things Software 3.0", "episode_title": "LLMs Everywhere: Running 70B models in browsers and iPhones using MLC \u2014 with Tianqi Chen of CMU / OctoML", "episode_image": "https://substackcdn.com/feed/podcast/1084089/92120159a90420fb694c09dfd0a904e8.jpg", "episode_transcript": " Hey everyone, welcome to the Late in Space podcast. This is Alessio, partner and CTO and resident at Decibel Partners, and I'm joined by my co-host, Sviks, writer and editor of Late in Space. Hey, and we are here with Tianqi Chen, or TQ as people call him, who is assistant professor in ML Computer Science at CMU Carnegie Mellon University, also helping to run Catalyst Group, also chief technologist of OctoML. You wear many hats. Are those your primary identities these days? Of course, of course. I'm also very enthusiastic open source, so I'm also a VPN piercing member of the Apache TVM project and so on. But yeah, these are the things I've been up to so far. You also created Apache TVM, XGBoost and MXNet, and we can cover any of those in any amount of detail. But maybe what's one thing about you that people might not learn from your official bio or LinkedIn on the personal side? Let me say, so normally I do a real life coding, even though I'm trying to run all those things. So one thing that I keep a habit on is I try to do sketchbooks. I have real sketchbooks to draw down the design diagrams and the sketchbooks. I keep sketching over the years and now I have like three or four of them. And it's kind of usually a fun experience of thinking the design through and also seeing how open source project evolves and also looking back at the sketches that we had in the past to see, you know, all these ideas really turn into code nowadays. How many sketchbooks did you get through to build all this stuff? I mean, if one person alone built one of those projects, it would be a very accomplished engineer like you built like three of these. What's that process like for you? Like is the sketchbook like the start and then you think about the code or like? Yeah. So usually I start sketching on high level architectures, right? And also in a project that works for over years, we also started to think about, you know, new directions like of course, generally where our language model comes in, how it's going to evolve. So normally I would say it takes like a one book a year roughly at that rate. It's usually fun to, I find it's much easier to sketch things out and then gives a more like a high level architectural guide for some of the future items. Have you ever published this sketchbooks? Because I think people would be very interested on at least on a historical basis, like this is the chart where XGBoost was born. Yeah, not really. I started sketching like after XGBoost. So that's the kind of missing piece, but a lot of design details in TVM are actually part of the books that I try to keep a record of. Yeah. We'll try to publish them and publish something in the show notes. Maybe you can grab a little snapshot for visual aid. Sounds good. Yeah. And yeah, talking about XGBoost, so a lot of people in the audience might know it's a gradient boosting library, probably the most popular out there. And it became super popular because many people started using them in like a machine learning competitions. And I think there's like a whole Wikipedia page of like all state of the art models that use XGBoost and like it's a really long list. When you were working on it, so we just had Tridau, who's the creator of a flash attention on the pockets. And I asked him this question. It's like when you were building flash attention, did you know that like almost any transform race model will use it? And so I asked the same question to you when you were coming up with XGBoost, like, could you predict it would be so popular or like what was the creation process? And when you published there, what did you expect? We have no idea. Like actually, the original reason that we built our library is that at that time, deep learning just came out. Like that was the time where Alex and I just came out. And one of the ambition mission that myself and my advisor Carl Skastrin, then is we want to think about, you know, try to test the hypothesis. Can we find alternatives to deep learning models? Because then, you know, there are other alternatives like, you know, support vector machines, linear models, and of course tree-based models. And our question was, if you build those models and feed them with big enough data, because usually like one of the key characteristics of deep learning is that they are taking a lot of data, right? So we will be able to get the same amount of performance. That's a hypothesis we're setting on our test. Of course, if you look at now, right, that's a wrong hypothesis. But as a byproduct, what we find out is that, you know, most of the gradient boosting library out there is not efficient enough for us to test that hypothesis. So I haven't had quite a bit of experience in the past of building gradient boosting trees and their variants. So Effective Action Boost was kind of like a byproduct of that hypothesis testing. At that time, I'm also competing a bit in data science challenges. Like I worked on KDD Cup and then Kaggle become bigger. So I kind of think maybe it's becoming useful to others. One of my friends convinced me to try to do a Python binding of it. That tends to be like a very good decision, right? To effectively, usually when I build it, we feel like maybe a command line interface is okay. And then we had a Python binding, we have R bindings, and then I realized it started to get interesting. People started contributing different perspectives, visualizations, and so on. So we started to push a bit more on to building distributed support to make sure it works on any more platform and so on. And even at that time, when I talked to Carlos, my advisor, later, he said he never anticipated that we'll get to that level of success. And actually, why I pushed for gradient boosting trees, interestingly, at that time, he also disagreed. He thinks that maybe we should go for kernel machines then. And it turns out, you know, actually, we are both wrong in some sense and deep neural network was the king in the hill, but at least the gradient boosting direction getting to something fruitful. Interesting. I'm always curious when it comes to these improvements, what's the design process in terms of coming up with it? And how much of it is collaborative with other people that you're working with versus trying to be, you know, obviously, an academia is very paper-driven, kind of research-driven. I would say the extra boost improvement at that time point was more on like, you know, I'm trying to figure out, right? But it's combining lessons. Before that, I did work on some of the other libraries on like on matrix factorization. That was like my first open source experience. Nobody knows about it. Because you will find likely if you go and try to search for the package for SVD feature, you'll find some like a SVN repo somewhere. That's actually being used for some of the recommender system packages. So I'm trying to apply some of the previous lessons there and trying to combine them. The later projects like MXNet and then TVM is much, much more collaborative, you know, in a sense that. But of course, extra boost has become bigger, right? So when we started that project, it's myself and then we have, it's really amazing to see people coming in. Like we have Michael, who was a lawyer and now he works on the AI space as well on contributing visualizations. And then we have people from our community contributing different things. So it's actually boost even today, right? This community of committers driving the project. So it's definitely something collaborative and moving forward on getting some of the things continuously improved for our community. Let's talk a bit about TVM too, because we got a lot of things to run through in this episode. So I feel like at some point I'd love to talk about this comparison between sort of extra boost or sort of tree based type AI or machine learning compared to deep learning. Because I think there is a lot of interest around, I guess, merging the two disciplines, right? And we can talk more about that. I don't know whether to insert that, by the way. So we can come back to it later. Yeah, that's good now. Yeah, actually, when I said, you know, when we test the hypothesis, the hypothesis is kind of, I would say it's partially wrong, right? Because the hypothesis we want to test now is, you know, can you run tree based models on image classification tasks? Where deep learning is certainly the no brainer right now today, right? But if you try to run it on tabular data, still, like you'll find that most people opt for tree based models. And that's the reason for that, in a sense that, you know, when you are looking at tree based models, the decision boundaries are naturally rules that you're looking at, right? And they also have nice properties like, you know, being able to be agnostic to scale of input and be able to automatically compose feature together. And I know there are attempts, there are attempts on building neural network models that works for tabular data. And I also sometimes follow them. I do feel like it's good to have a bit of diversity in a modeling space. Actually actually, when we're building TVM, we build cost models for the programs. Actually, we are using actually boost for that as well. I still think tree based model is going to be quite relevant because first for the, it's really to get it work out of box. And also, you will be able to get a bit of interoperability and control like, you know, monotonicity and so on. So yes, it's still going to be relevant. I also sometimes keep coming back to think about are there possible improvements that we can build on top of these models? And definitely, I feel like it's a space that can have some of a potential in the future. Are there any current projects that you would call out as promising in terms of merging the two, I guess, directions? I think there are projects that tries to bring like transformer type model for tabular data. I don't remember specifics of them, but I think even nowadays, if you look at the people's, what people are using, like tree based models do one of their toolkit. So I think maybe eventually it's not even a replacement, it will be just on some of models that you can call. Perfect. Next step, about three years after XG Boost, you built this thing called TBM, which is now a very popular compiler framework for models. Let's talk about, so this came out about at the same time as Onyx. So I think it would be great if you could maybe give a little bit of an overview of like how to do things work together, because it's kind of like the model then goes to Onyx, then goes to the TBM, but I think a lot of people don't understand that then once this. I can get a bit of backstory on that. So actually that's kind of an ancient history. Before XG Boost, I worked on deep learning for like two years or three years. I got a master before I started my PhD and during my master, my CSERs focused on applying convolutional restricted bozeman machine for ImageNet classification. That is the thing I'm working on. And that was before AlexNet moment. So effectively I had to handcraft NVIDIA CUDA kernels on, I think it was GTX 2070 card. I have a 22070 card. It took me about six months to get one model working. And eventually that model is not so good and we should have picked a better model. But that was like an ancient history that when I really get me into this deep learning field and of course, eventually we find it didn't work out. So in my master, I ended up working on recommend assistant, which got me a paper and I applied and got a PhD. But I always want to come back to work on the deep learning field. So after XG Boost, I think I started to work with some of the folks on this particular ImageNet. At that time it was like the frameworks are CAFE, Ciano, Torch, PyTorch haven't yet come out and we're really working hard to optimize for performance on GPUs. At that time I find it's really hard even for NVIDIA GPU. It took me six months and then it's amazing to see on different hardwares how hard it is to go and optimize code for the platforms that are interesting. So that gets me thinking, can we build something more generic and automatic? So that I don't need an entire team of so many people to go and build those frameworks. So that's the motivation of starting working on TVM. There is really too low the bar of machinery engineering needed to support deep learning models on the platforms that we're interested in. So I think it started a bit earlier than ONNX, but Wwise got announced, I think it's in a similar time period at that time. So overall how it works is that TVM, you will be able to take a subset of machine learning programs that are represented in what we call a computational graph. Nowadays we can also represent it as a loop level program. We ingest from our machine learning models. Usually you have model formats on ONNX, or in PyTorch they have FX chaser that allows you to chase the FX graph. And then it goes through TVM. We also realized that OES needs to be more customizable so it will be able to perform some of the compilation optimizations like fusion operator together, doing smart memory plannings and more importantly, generate low level code. So that works for NVIDIA and also is portable to other GPU backhands, even non-GPU backhands out there. Yeah. So that's the project actually has been my primary focus over the past few years. And it's great to see how it started from where I think we are the very early initiator of machine learning compilation. I remember I was visiting one of the students and asked me, are you still working on deep learning frameworks? I tell them that I'm working on ML compilation. And they said, okay, compilation, that sounds very ancient. It sounds like a very old field and why are you working on this? And now it's starting to get more traction. Like if you say Torch compiler and other things, I'm really glad to see this field starting picking up and also we have to continue innovating. I think the other thing that I noticed is it's kind of like a big jump in terms of area of focus to go from like XGBoost to like TVM. It's kind of like a different part of the stack. Why did you decide to do that? And I think the other thing about compiling to like different GPUs and eventually CPUs too, did you already see kind of like some of the strain that models could have, like just being focused on one runtime, only being on Cuda and like that? And how much of that went into it? I think it's less about trying to get impact, more about wanting to have fun. I like to hack code. I had great fun hacking Cuda code. And of course being able to generate Cuda code is cool, right? But now after being able to generate Cuda code, okay, by the way, you can do it on other platforms. Isn't that amazing? So it's more of that attitude to get me started on this. And also I think when we look at different researchers, myself is more like I think a problem solver type. So I like to look at a problem and say, okay, what kind of tools we need to solve that problem. So regardless, it could be building better models from where we build XGBoost, we build certain regularizations into it so that it's more robust. It also means building system optimizations, writing low level code, maybe trying to write assembly and build compilers and so on. So as long as they solve the problem, definitely go and try to do them together. And I also say it's a common trend right now. Like if you want to be able to solve machine learning problems, it's no longer an algorithm layer, right? You kind of need to solve it from both algorithm data and systems angle. And this entire field of machine learning system, I think it's kind of emerging and that's now a conference around it. And it's really good to see a lot more people are starting to look into this. Are you talking about ICML or something else? So machine learning and systems, right? So not only machine learning, but machine learning and system. So that's a conference called MLSys. It's a definitely smaller community than ICML, but I think it's also emerging and growing community where people are talking about what is the implications of building systems for machine learning and how do you go and optimize things around it and co-design models and systems together. Yeah. And you were area chair for ICML in Europe as well. So you've just had a lot of conference and community organization experience. Is that also like an important part of your work? Well, it's kind of expected for academic, if you hold an academic job, right? You need to do services for the community. Okay, great. Your most recent venture in MLSys is going to the phone with MLCLM. You announced this in April. I have it on my phone. It's great. I'm running Lamutu, Vicu\u00f1a. I don't know what other models you offer, but maybe just kind of describe your journey into MLC. And I don't know how this coincides with your work at CMU. Is that some kind of outgrowth? I think it's more like a focused effort that we want in the area of machine learning compilation. So it's kind of related to what we built in TVM. So when we built TVM was five years ago. And a lot of things happened. We built end-to-end machine learning compiler that works. The first one that works. But then we captured a lot of lessons there. So then we are building a second iteration called TVM Unity that allows us to be able to allow ML engineers to be able to quickly capture the new model and how we're demanding building optimizations for them. And MLCLM is kind of like an MLC. It's more like a vertical driven organization that we go and build tutorials and go and build projects like LIM2 solutions to really show you can take machine learning compilation technology and apply it and bring something fun forward. So yes, it runs on phones, which is really cool. But the goal here is not only making it run on phones. The goal is making it deploy universally. So we do run on Apple M2 Max, the 17 billion models. Anyway, on a single batch inference, more recently on CUDA, we get, I think, the most best performance we can get out there already on the 4-bit inference. And then actually, as I alluded earlier before the podcast, we just had a result on AMD. And on a single batch, actually, we can get the latest AMD GPU, it is the consumer card. It can get to about 80% of the 4090. It's the media's best consumer card out there. So it's not yet on par, but thinking about how our diversity and what you can enable and the previous things you can get on that card is really amazing what you can do with this kind of technology. So one thing I'm a little bit confused by is that most of these models are in PyTorch, but you're running this inside a TVM. I don't know, was there any fundamental change that you needed to do or was this basically the fundamental design of TVM? So the idea is that, of course, it comes back to program representation. So effectively, TVM have this program representation called TVM script that contains more like computational graph and operational representation. So yes, initially, we do need to take a bit of effort of bringing those models onto the program representation that TVM support. Usually there are a mix of ways, depending on kind of model you're looking at. So for vision models and stable diffusion models, usually we can adjust to tracing that takes PyTorch model onto TVM. That part is still being robustified so that we can bring more model in. On language model cards, actually what we do is we directly build some of the model constructors and try to directly map from hugging phase models. The goal is if you have a hugging phase configuration, we will be able to bring that in and apply optimization on them. So one fun thing about model compilation is that your optimization don't happen only at a source language. From where you write PyTorch code, you just go and try to use better fields operator at a source code level. Torch compile might help you do a bit of things in there. In most of model compilations, it not only happens at the beginning stage, but we also apply generic transformations in between, also through a Python API. So you can tweak some of that. So that part of optimization helps a lot of uplifting in getting both performance and also portability on the environment. And another thing that we do have is what we call universal deployment. So if you get the MO program into this TVM script format where there are functions that takes in tensor and output tensor, we will be able to have a way to compile it so that you will be able to load the function in any of the language runtime that TVM supports. So if you could load it in JavaScript, and that's a JavaScript function that you can take in tensors and output tensors if you're loading Python, of course, and C++ and Java. So the goal there is really bring the MO model to the language that people care about and be able to run on the platforms they like. It strikes me that I've talked to a lot of compiler people, but you don't have a traditional compiler background. You're inventing your own discipline called machine learning compilation, or MLC. Do you think that this will be a bigger field going forward? First of all, I do work with the people working on compilation as well. So we're also taking inspirations from a lot of early innovations in the field, like for example, TVM initially, we take a lot of inspirations from Halide, which is this image processing compiler. And of course, since then, we have evolved quite a bit to focus on the machine learning related compilations. If you look at some of our conference publications, you'll find the machine learning compilation is already kind of a subfield. So if you look at papers in both machine learning venues, the MO-SYS conferences, of course, and also system venues, every year there will be papers around machine learning compilation. And in the compiler conference called CGO, there's a C4MO workshop that also kind of trying to focus on this area. So definitely it's already starting to gain interaction and becoming a field. I wouldn't claim that I invented this field, but definitely I helped to work with a lot of folks there. And I try to bring a perspective. Of course, trying to learn a lot from the compiler optimizations, as well as trying to bring in knowledge in machine learning and assistance together. So we had George Hotz on the podcast a few episodes ago, and he had a lot to say about AMD and their software. So when you think about TVM, are you still restricted in a way by the performance of the underlying kernel, so to speak? If your target is a CUDA runtime, you still get better performance, no matter... Like TVM kind of helps you get there, but then that level you don't take care of. There are two parts in here. So first of all, there is the lower level runtime, like CUDA runtime. And then actually for Nvidia, a lot of the mood came from their libraries, like Cutler, CUDN, those library optimizations. And also for specialized workloads, actually you can specialize them, because a lot of cases you'll find that if you go and do benchmarks, it's very interesting. Like two years ago, if you try to benchmark ResNet, for example, usually the Nvidia library will give you the best performance. It's really hard to beat them. But as soon as you start to change the model to something, maybe a bit of a variation of ResNet, not for the image net detections, but for lane detection and so on, there will be some room for optimization because people sometimes overfit to benchmarks. These are people who go and optimize things, right? So the people overfit the benchmarks. So that's the largest barrier, like being able to get low level kernel libraries, right? In that sense, the goal of TVM is actually we try to have a generic layer to both, of course, leverage libraries when available, but also being able to automatically generate libraries when possible. So in that sense, we are not restricted by the libraries that they have to offer. That's why Firmware will be able to run Apple M2, a web GPU where there's no library available because we are kind of like automaking generating libraries. That makes it easier to support less well-supported hardware, right? Like Firmware web GPU is one example from a runtime perspective. AMD, I think before their lock-in driver was not very well supported. Recently they are getting good. But even before that, we will be able to support AMD through this GPU graphics back and kind of Vulkan, which is not as performant, but it gives you a better portability across those hardware. And I know we got other MLC stuff to talk about like WebLM, but I want to wrap up on the optimization that you're doing. So there's kind of four core things, right? Kernel fusion, which we talked a bit about in the flash attention episode and the tiny grab one. Pre-planning and loop optimization, I think those are like pretty self-explanatory. I think the one that people have the most questions about is like... Can you quickly explain those? Yeah, go for it. So they are kind of different things, right? Kernel fusion means that if you have operator convolutions or in the case of transformer like MLP, you have other operators follow that, right? You don't want to launch two GPU kernels. You want to be able to put them together in a smart way, right? Because the memory planning is more about, hey, if you run like Python code, every time when you generate a new array, you are effectively allocating a new piece of memory, right? Of course, PyTorch and other framework try to optimize for you. So they are a smart memory allocator behind the scene. But actually in a lot of cases, it's much better to statically allocate and plan everything ahead of time. And that's where like a compiler can come in. First of all, actually for language model, it's much harder because dynamic shape. So you need to be able to what we call symbolic shape tracing. So we have like a symbolic variable that tells you like the shape of the first tensor is n by 12. And the shape of the third tensor is also n by 12, or maybe it's n times 2 by 12. Although you don't know what n is, right? But you will be able to know that relation and be able to use that to reason about fusion and other decisions. So besides this, I think loop transformation is quite important. And it's actually non-traditional. Like originally, if you simply write a code and you want to get a performance, it's very hard. If you write matrix multiply, the simplest thing you can do is you do for ijk cij plus equal aik times bik. But that code is 100 times slower than the best available code that you can get. So we do a lot of transformation, like being able to take the original code, trying to put things into shared memory, and making use of tensor calls, making use of memory copies. And all these things, we also realize that we cannot do all of them. So we also make the ML compilation framework as a Python package so that people will be able to continuously improve that part of engineering in a more transparent way. So we find that's very useful actually for us to be able to get good performance very quickly on some of the new models, like when Lamato came out, we'll be able to go and look at the whole, here's the bottleneck, and we can go and optimize those. And then the board one being a weight quantization. So everybody wants to know about that. And just to give people an idea of the memory saving, if you're doing FB32, it's like four bytes per parameter, and A, it is like one byte per parameter. So you can really shrink down the memory footprint. What are some of the trade-offs there? How do you figure out what the right target is? And what are the precision trade-offs to? Right now, a lot of people like also, we also mostly use Int4 now for language models. So that really shrinks things down a lot. And more recently, actually, we started to think that at least in MOC, we don't want to have a strong opinion on what kind of correlation we want to bring because there are so many research in the field. So what we can do is we can allow developers to customize the correlation they want, but we still bring the optimum code for them. So we are working on this item called Bring Your Own Quantization, in fact, to be able to hopefully, MOC will be able to support more correlation format. And definitely, I think there's an open field that's being explored. Can you bring more sparsities? Can you quantize activations as much as possible and so on? It's going to be something that's going to be relevant for quite a while. You mentioned something I wanted to double back on, which is most people use Int4 for language models. This is actually not obvious to me. Are you talking about the GGML type people or even the researchers who are training the models also using Int4? Sorry. So I'm mainly talking about inference, not training. So when you're doing training, of course, Int4 is harder. Maybe you could do some form of mixed type precisions for inference. I think Int4 is kind of like a lot of cases you will be able to get away with Int4 in a lot of cases. And actually, that does bring a lot of savings in terms of the memory overhead and so on. Yeah, that's great. Let's talk a bit about maybe the GGML, then there's Mojo. How should people think about MLC? How do all these things play together? I think GGML is focused on kind of like model level implementation and improvements. Mojo is like a language superset. You're more at the compiler level. Do you all work together? Do people choose between them? So I think in this case, I think it's great to say the ecosystem becomes so rich with so many different ways. So in our case, I would say GGML is more like you are implementing something from scratch in C. So that gives you the ability to go and customize each of a particular hardware backend. But then you will need to write from a CUDA kernels and you write optimally from AMD and so on. So the kind of engineering effort is a bit more broadened in that sense. Mojo, I have not looked at specific details yet. I think it's good to start to say it's a language, right? I believe there will also be machine learning compilation technologies behind it. So it's good to say interesting place in there. In the case of MOC, our case is that we do not want to have an opinion on which language people want to develop, deploy, and so on. And we also realized that actually there are two phases. We want to be able to develop and optimize your model. By optimization, I mean really bring best CUDA kernels and do some of the machine learning engineering in there. And then there's a phase where you want to deploy it as a part of the app. So if you look at the space, you'll find that GGMO is more like, I'm going to develop optimize in the C language, and the most of the low level languages they have. And Mojo is that you want to develop and optimize in Mojo. You deploy in Mojo. That's the philosophy they want to push for. In the MOC case, we find that actually for the developed models, machine learning community lacks Python. Python is a language that you should focus on. So in the case of MOC, we really want to be able to enable not only be able to just define your model in Python. That's very common. But also do ML optimization, like engineering optimization, CUDA kernel optimization, memory planning, all those things in Python that makes you customizable and so on. But when you do deployment, we realize that people want a bit of a universal flavor. If you are a web developer, you want JavaScript. If you are maybe an embedded system person, maybe you would prefer C++ or C or Rust. And people sometimes do like Python in a lot of cases. So in the case of MOC, we really want to have this vision of, you optimize, build a generic optimizations in Python, and then you deploy that universally onto the environments that people like. That's a great perspective and comparison, I guess. One thing I wanted to make sure that we cover is that I think you are one of these emerging set of academics that also very much focus on your artifacts of delivery. Of course. Something we talked about to tree is that he was very focused on his GitHub. And obviously you treated XGBoost like a product, and then now you're publishing an iPhone app. Okay. Yeah. What is his thinking about academics getting involved in shipping products? I think there are different ways of making impact. In a way, there are academics that are writing papers and building insights for people so that people can build product on top of them. In my case, I think the particular field I'm working on, machine learning systems, I feel like really we need to be able to get it to the hand of people so that really we see the problem, right? And we show that we can solve the problem. And it's a different way of making impact. And there are academics that are doing similar things, like if you look at some of the people from Berkeley, a few years, they will come up with big open source projects. Certainly, I think it's just a healthy ecosystem to have different ways of making impacts. And I feel like really be able to do open source and work with open source community is really rewarding because we have real problem work on when we build our research, actually those research bring together and people will be able to make use of them. We also start to see interesting research challenges that we wouldn't otherwise say, right? If you're just trying to do a prototype and so on. So I feel like it's something that is one interesting way of making impact, make contributions. You definitely have a lot of impact there. And having experienced publishing Mac stuff before, the Apple App Store is no joke. It is the hardest human compilation effort. So one thing that we definitely wanted to cover is running in the browser. You have a 70 billion parameter model running in the browser. Can you just talk about how? Yeah, of course. I think that there are a few elements that need to come in. First of all, we do need a MacBook, the latest one, like M2 Max, because you need the memory to be big enough to cover that. So for a 17 billion model, it takes you about, I think, 50 gigas of RAM. So the M2 Max, the upper version, will be able to run it. And it also leverages machine learning compilation. Again, what we are doing is the same. Whether it's running on iPhone, on server cards, GPUs, on AMDs, or on MacBook, we all go through that same MOC pipeline. Of course, in certain cases, maybe we'll do a bit of customization iteration for either ones. And then it runs on the browser runtime, this package of Web IOM. So what we do is we will take that original model and compile it to what we call Web GPU. And then the Web IOM will be picked up. And the Web GPU is this latest GPU technology that major browsers are shipping right now. So you can get it in Chrome for example already. It allows you to be able to access your native GPUs from a browser. And then effectively that language model is just invoking the Web GPU kernels through there. So actually, when the Lama 2 came out, initially we asked the question about, can you run 17 billion on a MacBook? That was the question we're asking. So first, we actually, Jun Lu, who is the engineer pushing this, he got 17 billion on a MacBook. We had a CLI version. So in the MOC, you will be able to run through metal accelerators. So effectively, you use the metal programming language to get the GPU acceleration. So we find, okay, it works for the MacBook. Then we ask, we had a Web GPU back and why not try it there? So we just tried it out. And it's really amazing to see everything up and running. And actually, it runs smoothly in that case. So I do think there are some kind of interesting use cases already in this, because everybody has a browser. You don't need to install anything. I think it doesn't make sense yet to really run a 17 billion model on a browser, because you kind of need to be able to download the weight and so on. But I think we're getting there. Effectively, the most powerful models, you will be able to run on consumer devices. It's kind of really amazing. And also, in a lot of cases, there might be use cases, for example, if I'm going to build a chatbot that I talk to it and it answers questions, maybe some of the component, like the voice to text could run on the client side. So there are a lot of possibilities of being able to have something hybrid that contains the edge component, something that runs on our server. Do these browser models have been a way for applications to hook into them? So if I'm using, say, you can use OpenAI or you can use the local model. Of course. So right now, actually, we are building... So there's an NPM package called WebILM. So that you will be able to, if you want to embed it onto your web app, you will be able to directly depend on WebILM. You will be able to use it. We are also having a REST API that's OpenAI compatible. So that REST API, I think right now, it's actually running on native backend. So that from a CUDA server is faster to run on native backend. But also, we have a web GPU version of it that you can go and run. So yeah, we do want to be able to have easy integrations with existing applications. And OpenAI API is certainly one way to do that. Yeah, this is great. I actually did not know there's an NPM package that makes it very, very easy to try out and use. I want to actually... One thing I'm unclear about is the chronology. Because as far as I know, Chrome shipped web GPU the same time that you shipped WebILM. Okay, yeah. So did you have some kind of secret chat with Chrome? The good news is that Chrome is doing a very good job of trying to have early release. So although the official shipment of the Chrome web GPU is the same time of WebILM, actually, you will be able to try out Web GPU technology in Chrome. There are unstable versions called Canary. I think as early as two years ago, that's a web GPU version. Of course, it's getting better. So we had TVM-based web GPU backend two years ago. Of course, at that time, there's no language models. It's running on less interesting, well, still quite interesting models. And then this year, we really started to see it's getting matured and performance keeping up. So we have a more serious push of bringing the language model compatible runtime onto the web GPU. I think you agree that the hardest part is the model download. Has there been conversations about a one-time model download and sharing between all the apps that might use this API? That is a great point. I think it's already supported kind of already. In some sense, when we download the model, WebILM will cache it onto a special Chrome cache. So if a different web app uses the same WebILM JavaScript package, you don't need to re-download the model again. So there's already something there. But of course, you have to download the model once, at least, to be able to use it. Okay. One more thing just in general before we're about to zoom out to the OctoAI. Just the last question is, you're not the only project working on, I guess, local models, alternative models. There's GPT4ALL, there's OLAMA that just recently came out, and there's a bunch of these. What would be your advice to them on what's a valuable problem to work on and what is just thin wrappers around GDML? What are the interesting problems in this space, basically? I think making API better is certainly something useful. In general, one thing that we do try to push very hard on is this idea of easier universal deployment. So we are also looking forward to actually have more integration with the MOC. That's why we're trying to build API WebILM, so we're also looking forward to collaborate with all those ecosystems, working support to bring in models more universally and be able to also keep up the best performance when possible, here, more push-button way. So as we mentioned in the beginning, you're also the co-founder of OctoML, recently OctoML released OctoAI, which is a compute service, basically focuses on optimizing model runtimes and acceleration and compilation. What has been the evolution there? So Octo started as kind of like a traditional MLOps tool, where people were building their own models and you help them on that side. And then it seems like now most of the market is shifting to starting from pre-trained generative models. So what has been that experience for you and what you've seen the market evolve and how did you decide to release OctoAI? One thing that we found out is that on one hand, it's really easy to go and get something up and running. So if you start to consider there's so many possible availabilities and scalability issues and even integration issues, things are becoming kind of interesting and complicated. So we really want to make sure to help people to get that part easy. And now a lot of things, if we look at the customers we talked to in the market, certainly generative AI is something that is very interesting. So that is something that we really hope to help elevate. And also building on top of technology we build to enable things like a portability across hardware and you will be able to not worry about the specific details, just focus on getting the model out. We'll try to work on infrastructure and other things that helps on the other end. And when it comes to getting optimization on the runtime, I see when we run an early adopters community and most enterprises' issue is how to actually run these models. Do you see that as one of the big bottlenecks now? I think a few years ago it was like, well, we don't have a lot of machine learning talent. We cannot develop our own models. Versus now there's these great models you can use, but I don't know how to run them efficiently. That depends on how you define by running. On one hand it's easy to download, like you download it, you run on a laptop, but then there's also different decisions. What if you are trying to serve larger user requests? What if that request changes? What if availability of hardware changes? Right now it's really hard to get the latest hardware on video, unfortunately, because everybody's trying to work on the things using the hardware that's out there. I think when the definition of run changes, there are a lot more questions around things. And also in a lot of cases, it's not only about running models, it's also about being able to solve problems around them. How do you manage your model locations? And how do you make sure that you get your model close to your exclusion environment more efficiently? Definitely a lot of engineering challenges out there that we hope to elevate. And also if you think about our future, definitely I feel like right now the technology, given the technology and the kind of hardware availability we have today, we would need to make use of all the possible hardware available out there. That includes a mechanism for cutting down costs, bringing something to Edge and Cloud in a more natural way. So I feel like this is a very early stage of where we are, but it's already good to say a lot of interesting progress. Yeah, that's awesome. I don't know how much you can go in depth into it, but what does it take to actually abstract all of this from the end user? They don't need to know what GPUs you run, what cloud you're running them on. You take all of that away. What was that like as an engineering challenge? So I think there are engineering challenges. In fact, first of all, you will need to be able to support all the kind of hardware back end you have. On one hand, if you look at the media library, you'll find very surprisingly, not too surprisingly, most of the latest libraries work well on the latest GPU. But there are other GPUs out there in the cloud as well. So certainly being able to have know-hows and being able to do model optimization is one thing. Also infrastructures on being able to scale things up, locate models. And in a lot of cases, we do find that on typical models, it also requires kind of vertical iterations. So it's not about you build a silver bullet and that silver bullet is going to solve all the problems. It's more about we're building a product, we work with the users and we find out there are interesting opportunities in a certain point and when our engineer will go and solve that and it will automatically reflect it in the service. Awesome. We can jump into the lightning round until I don't know, Sean, if you have more questions or TQ, if you have more stuff you wanted to talk about that we didn't get a chance to touch on. Yeah, we have talked a lot. Yeah. We always like to ask, do you have a commentary on other parts of AI and ML that is interesting to you? So right now, I think one thing that we are really pushing hard for is this question about how far can we bring open source? I'm kind of like a hacker and I really like to put things together. So I think this unclear in the future of what the future of AI looks like. On one hand, it could be possible that you just have a few big players, you just try to talk to those bigger language models and that can do everything. On the other hand, one of the things that Welling Academic had really excited pushing for, that's one reason why I'm pushing for MLC, is that can we build something where you have different models, you have personal models that knows the best movie you like, but you also have bigger models that maybe know more and that you get those models intact with each other and be able to have a wide ecosystem, AI agents that helps each person while still being able to do things like personalization. Some of them can run locally, some of them, of course, running on the cloud and how do they interact with each other? So I think that is a very exciting time where the future is yet undecided, but I feel like there are some things we can do to shape that future as well. One more thing, which is something I'm also pursuing, which is, and this kind of goes back into predictions, but also back in your history, do you have any idea or are you looking out for anything post-transformers as far as architecture is concerned? I think in a lot of these cases, you can find there are already problems in models for long contexts, right? There are space-based models where a lot of some of my colleagues from Albert who he worked on, there's HIPAA models, right? And there is an open source version called RWKV. It's like a recurrent model that allows you to summarize things. Actually we are bringing RWKV to MOC as well, so maybe you will be able to see one of the models. We actually recorded an episode with one of the RWKV core members. It's unclear because there's no academic backing, it's just open source people. Oh, I see. So you like the merging of recurrent networks and transformers? I do love to see this model space continue growing, right? And I feel like in a lot of cases, it's just that attention mechanism is getting changed in some sense. So I feel like definitely there are still a lot of things to be explored here. And that is also one reason why we want to keep pushing machine learning compilation because one of the things we are trying to push in was productivity for machine learning engineering so that as soon as some of the models came out, we will be able to empower them onto those environments that's out there. Yeah. That's a really good mission. Okay. Very excited to see that RWKV and these space model stuff. I'm hearing increasing chatter about that stuff. Okay. Lightning round is always fun. I'll take the first one. Acceleration. What has already happened in AI that you thought would take much longer? Emergence of more like a conversation chatbot ability is something that kind of surprised me before it came out. This is like one piece that I feel originally I thought it would take much longer, but yeah, it happens. It's funny because the original Eliza chatbot was something that goes all the way back in time, right? And then we just suddenly came back again. Yeah. It's always too interesting to come back, but with a kind of a different technology in some sense. What about the most interesting unsolved question in AI? That's a hard one, right? So I can tell you what kind of I'm excited about. So I think that I have always been excited about this idea of continuous learning and lifelong learning in some sense. So how AI continues to evolve with the knowledge that's been there. Seems that we're getting much closer with all those latest recent technologies. So being able to be able to develop systems support and be able to think about how AI continues to evolve is something that I'm really excited about. So specifically just to double click on this, are you talking about continuous training? That's like a training. I feel like training adaptation and it's all similar things. You want to think about the entire life cycle, right? The life cycle of collecting data, training, fine tuning, and maybe have your local context that getting continuously curated and feed onto models. So I think all these things are interesting and relevant here. Yeah. And I think this is something that people are really asking. Right now we have moved a lot into the pre-training phase and off the shelf model downloads and stuff like that, which seems very counterintuitive compared to the continuous training paradigm that people want. So I guess the last question would be for takeaways. What's basically one message that you want every listener, every person to remember today? I think it's getting more obvious now, but I think one of the things I always want to mention in my talks is that when you think about AI applications, originally people think about algorithms a lot more. Algorithms and models, they are still very important, but usually when you build AI applications, takes both algorithms side, the system optimizations and the data curations. So it takes a connection of so many facades to be able to bring together AI system and be able to looking at from that whole perspective is really useful. Always start to build modern AI applications. I think it's going to continue going to be more important in the future. Thank you for showing the way on this and honestly just making things possible that I thought would take a lot longer. So thanks for everything you've done. Thank you for having me. Yeah. Thanks for coming on TQ. Please subscribe."}, "podcast_summary": "Hey everyone, welcome to this episode of the Late in Space podcast. In this episode, hosts Alessio and Svek speak with Tianqi Chen, an assistant professor in ML Computer Science at Carnegie Mellon University and co-founder of OctoML. They discuss Tianqi's various roles and projects, including his work on Apache TVM, XGBoost, and MXNet. Tianqi also talks about his love for sketching and how it helps him design and develop open source projects. They go on to discuss the creation of XGBoost and its surprising success, as well as the evolution of machine learning compilation and the challenges and opportunities it presents. Tianqi also shares his thoughts on the future of AI and machine learning, including the importance of continuous learning and bringing together algorithms, system optimizations, and data curation. The episode concludes with a discussion on OctoAI and the challenges of running models efficiently in the browser. The hosts also inquire about the future of AI architectures and the potential for merging transformer models with tree-based models. Overall, it is an insightful conversation that highlights Tianqi's important contributions to the field of AI and machine learning, as well as his vision for the future.", "podcast_guest": "Tianqi Chen", "podcast_highlights": "In this podcast transcript, highlights include:\n\n- TQ sketching design diagrams in his sketchbooks and how they have evolved over time.\n- The motivation behind building XGBoost and its unexpected popularity.\n- The emergence of machine learning compilation as a field and its importance in optimizing model runtimes.\n- The challenges and advantages of running models in the browser with WebGPU.\n- The need for open source technology to shape the future of AI and machine learning.\n- The potential of merging recurrent networks and transformers in future model architectures.\n- The excitement around continuous learning and lifelong learning in AI applications.\n- The importance of considering algorithms, system optimizations, and data curation when building AI applications."}